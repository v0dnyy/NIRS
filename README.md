## Постановка задачи:
Разработка методов генерации adversarial примеров и оценка их воздействия на предобученные нейросетевые модели для классификации и обнаружения людей, с акцентом на анализ изменений в точности(accuracy) моделей при использовании как стандартного тестового набора данных, так и набора данных, состоящего из adversarial примеров, сгененрированных с помощью различных методов атак при различных гиперпараметрах.


### Иследуемые предобученные модели.
Для анализа использовались модели, предобученные на наборе данных ImageNet, из модуля [torchvision](https://pytorch.org/vision/stable/) библиотеки [PyTorch](https://pytorch.org/).

Для задачи обнаружения объектов в модуле доступны следующие модели:
- [**Faster R-CNN**](https://pytorch.org/vision/stable/models/faster_rcnn.html);
- [**RetinaNet**](https://pytorch.org/vision/stable/models/retinanet.html);
- [**SSD**](https://pytorch.org/vision/stable/models/ssd.html);
- [**SSDlite**](https://pytorch.org/vision/stable/models/ssdlite.html).

В backbone моделей, приведенных выше, используютя следующие модели классификации:
- [**ResNet50**](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.resnet50) с механизмом Feature Pyramid Network(FPN) для более эффективного извлечения признаков;
- [**MobileNet V3**](https://pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.mobilenet_v3_large);
- [**VGG-16**](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html#torchvision.models.vgg16).

Было принято решение использовать предобученные на наборе данных [ImageNet](https://image-net.org/index.php) модели классификации, представляющие backbone моделей обнаружения объектов, для проведения исследоваения.


### Тестовый набор данных.
Для оценки точности предобученнах моделей, был собран и аннотирован тестовый набор данных, состоящий из 200 изображений различных классов набора данных ImageNet.

Набор данных представлен в репозитории -- [тестовый набор данных](dataset/)

Папка [images](dataset/images/) содержит изображения, входящие в тестовый набор данных.
Файл [labels.csv](dataset/labels.csv) содержит в себе аннотации к изображениям.  


### Оценка точности предобученных моделей на тестовом наборе данных.
- **ResNet50**<br>
  Точность модели на тестовом наборе: 96.00 %
- **MobileNet V3**<br>
  Точность модели на тестовом наборе: 89.50 %
- **VGG-16**<br>
  Точность модели на тестовом наборе: 83.00 %

#### Пример первых пяти изображений и их классов из тестового набора данных.

![Orig images and their classes](images/orig_imgs.png)

### Классификация состязательных атак (Adversarial Attacks).

Состязательная атака (adversarial attack) – это воздействие на нейронную сеть при помощи изменения исходных данных с целью получения неверного результата классификации.

Можно выделить 5 основных критериев классификации состязательных атак:

1. **Результат воздействия на систему**:
   - Изменение поведения;
   - Нарушение конфиденциальности.

2. **Вид атаки**:
   - Атака отравления;
   - Атака уклонения;
   - Атака BadNets.

3. **Наличие информации о модели**:
   - Белый ящик;
   - Черный ящик;
   - Серый ящик.

4. **Тип атаки**:
   - Целевая атака;
   - Нецелевая атака.

5. **Тип применимости**:
   - Виртуальная/цифровая;
   - Физическая.

#### Атаки изменения поведения

Атаки изменения поведения ставят своей целью изменить результат работы модели, то есть нарушить целостность системы. 

#### Атаки нарушения конфиденциальности

Атаки нарушения конфиденциальности ставят своей целью получение доступа к различным артефактам сети, таким как обучающие данные, архитектура, веса обученной модели и т.д. Также сюда относятся атаки, позволяющие узнать что-то об обучающей выборке или получить модель, которая копирует функционал исходной модели.

#### Атака отравления данных

Атака отравления данных – это задача двухуровневой оптимизации по измененным обучающим данным и по ответу обученной модели. Целью данной атаки является изменение ответа обученной модели.

При обучении модели на открытых наборах данных злоумышленники могут легко их «отравить». Злоумышленники заведомо дают ложный ответ в качестве верного, тем самым отравляя обучающую выборку. На открытых наборах данных экспериментально показано, что при полной доступности параметров модели можно внести в нее критические ошибки.

#### Атаки уклонения

Атаки уклонения (evasion attacks) представляют собой методики, с помощью которых злоумышленники могут вводить небольшие возмущения в входные данные, чтобы обмануть модель машинного обучения и заставить её выдать неправильные результаты. Эти атаки являются серьезной угрозой для безопасности и надежности систем, основанных на глубоких нейронных сетях.

#### Типы атак по доступности информации:

- Атаки с белым ящиком (white-box attacks)<br>
Атака, при которой злоумышленник имеет полный доступ к информации о модели, включая архитектуру, веса и данные обучения. В этом случае он может получить и использовать внутреннюю информацию модели, такую как веса модели и данные обучения. 
*Пример*: злоумышленник знает архитектуру нейронной сети, все параметры и имеет доступ к обучающему набору данных.
- Атаки с черным ящиком (black-box attacks)<br>
Атака, при которой злоумышленник не может получить никакую информацию о модели во время процесса генерации враждебных примеров и может использовать только выходные данные модели. В реальных сценариях получения внутренней информации о целевых системах невозможно, поэтому атаки с черным ящиком более применимы.
*Пример*: злоумышленник имеет доступ только к выходным данным модели (например, к предсказанным меткам или вероятностям), но не знает внутренней структуры или параметров модели.
- Атаки "серого ящика" (gray box)<br>
Атаки, сочетающие характеристики атак "белого" и "черного" ящиков. В этом случае атакующий располагает частичной информацией о модели, что делает такие атаки более реалистичными, но все же накладывает некоторые ограничения. Например, может быть известна архитектура модели или информация о данных, на которых модель была обучена, однако отсутствовать доступ к самим данным или обученным весам.

#### Типы атак по целенаправленности:

- Нецелевая атака<br>
Атаки, целью которых является изменение предсказания модели на любой другой класс, отличный от правильного. Эти атаки проще в реализации.
- Целевая атака<br>
Атаки, целью которых является вынудить модель выдать предсказание конкретного класса. Эти атаки сложнее в реализации, так как требуют более точных возмущений.

### **Реализация наиболее распространенных состязательных атак.**

#### Метод Быстрого Знака Градиента(FGSM)
Этот метод был описан в данных статьях:

-[Adversarial attacks in computer vision: a survey](https://link.springer.com/article/10.1007/s41965-024-00142-3)

-[Comprehensive Review on Advanced Adversarial Attack and Defense Strategies in Deep Neural Network](https://rsisinternational.org/journals/ijrias/DigitalLibrary/volume-8-issue-4/156-166.pdf)

-[Fooling deep neural detection networks with adaptive object-oriented adversarial perturbation](https://www.sciencedirect.com/science/article/abs/pii/S003132032100090X?via%3Dihub)

Это классический метод атаки белого ящика, который указывает, что из-за нелинейности глубоких нейронных сетей даже небольшое возмущение, добавленное к входу, достаточно для того, чтобы ввести нейронную сеть в заблуждение при классификации.

Файл [FGSM.py](FGSM.py) содержит функцию fsgm(), являющейся реализацией нецеловой атаки на изображение с помощью метода FGSM.

Метод был протестирован с различними значениями epsilon: 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3. Так как при значениях epsilon больше, чем 0.3, наличие шума становится отчетливо визуально отличимо.

- **ResNet50**<br>

  Точность модели на тестовом наборе при epsilon = 0.01 : 51.00

  Количество изображений, поменявших свой класс: 98
  ![image](images/fgsm/resnet50/1.png)

  Точность модели на тестовом наборе при epsilon = 0.05 : 44.50

  Количество изображений, поменявших свой класс: 111
  
  ![image](images/fgsm/resnet50/2.png)
  
  Точность модели на тестовом наборе при epsilon = 0.1 : 47.50

  Количество изображений, поменявших свой класс: 105
  
  ![image](images/fgsm/resnet50/3.png)
  
  Точность модели на тестовом наборе при epsilon = 0.15 : 52.00
  
  Количество изображений, поменявших свой класс: 96
  
  ![image](images/fgsm/resnet50/4.png)
  
  Точность модели на тестовом наборе при epsilon = 0.2 : 54.00
  
  Количество изображений, поменявших свой класс: 92
  
  ![image](images/fgsm/resnet50/5.png)
  
  Точность модели на тестовом наборе при epsilon = 0.25 : 51.50
  
  Количество изображений, поменявших свой класс: 97
  
  ![image](images/fgsm/resnet50/6.png)
  
  Точность модели на тестовом наборе при epsilon = 0.3 : 53.50
  
  Количество изображений, поменявших свой класс: 93
  
  ![image](images/fgsm/resnet50/7.png)
  
  График зависимости точности от параметра epsilon
  
  ![image](images/fgsm/resnet50/acc.png)

- **MobileNet V3**<br>

  Точность модели на тестовом наборе при eps=0.01 : 25.00

  Количество изображений, поменявших свой класс: 150

  ![image](images/fgsm/mobilenet/1.png)

  Точность модели на тестовом наборе при eps=0.05 : 17.50

  Количество изображений, поменявших свой класс: 165

  ![image](images/fgsm/mobilenet/2.png)

  Точность модели на тестовом наборе при eps=0.1 : 18.00

  Количество изображений, поменявших свой класс: 164

  ![image](images/fgsm/mobilenet/3.png)

  Точность модели на тестовом наборе при eps=0.15 : 20.50

  Количество изображений, поменявших свой класс: 159

  ![image](images/fgsm/mobilenet/4.png)

  Точность модели на тестовом наборе при eps=0.2 : 22.50

  Количество изображений, поменявших свой класс: 155

  ![image](images/fgsm/mobilenet/5.png)

  Точность модели на тестовом наборе при eps=0.25 : 22.50

  Количество изображений, поменявших свой класс: 155

  ![image](images/fgsm/mobilenet/5.png)

  Точность модели на тестовом наборе при eps=0.3 : 21.50

  Количество изображений, поменявших свой класс: 157

  ![image](images/fgsm/mobilenet/7.png)

  График зависимости точности от параметра epsilon

  ![image](images/fgsm/mobilenet/acc.png)
  
- **VGG-16**<br>

  Точность модели на тестовом наборе при eps=0.01 : 20.50
  
  Количество изображений, поменявших свой класс: 159
  
  ![image](images/fgsm/vgg16/1.png)
  
  Точность модели на тестовом наборе при eps=0.05 : 9.50
  
  Количество изображений, поменявших свой класс: 181
  
  ![image](images/fgsm/vgg16/2.png)
  
  Точность модели на тестовом наборе при eps=0.1 : 8.50
  
  Количество изображений, поменявших свой класс: 183
  
  ![image](images/fgsm/vgg16/3.png)
  
  Точность модели на тестовом наборе при eps=0.15 : 7.50
  
  Количество изображений, поменявших свой класс: 185
  
  ![image](images/fgsm/vgg16/4.png)
  
  Точность модели на тестовом наборе при eps=0.2 : 7.50
  
  Количество изображений, поменявших свой класс: 185
  
  ![image](images/fgsm/vgg16/5.png)
  
  Точность модели на тестовом наборе при eps=0.25 : 8.50
  
  Количество изображений, поменявших свой класс: 183
  
  ![image](images/fgsm/vgg16/6.png)
  
  Точность модели на тестовом наборе при eps=0.3 : 8.50
  
  Количество изображений, поменявших свой класс: 183
  
  ![image](images/fgsm/vgg16/7.png)
  
  График зависимости точности от параметра epsilon

  ![image](images/fgsm/vgg16/acc.png)

#### Итеративный Метод Быстрого Знака Градиента(I-FGSM)
Этот метод был описан в данной статье:

-[Fooling deep neural detection networks with adaptive object-oriented adversarial perturbation](https://www.sciencedirect.com/science/article/abs/pii/S003132032100090X?via%3Dihub)

Метод I-FGSM (Iterative Fast Gradient Sign Method) является усовершенствованной версией FGSM. В отличие от FGSM, который выполняет одно обновление на основе градиента функции потерь, I-FGSM выполняет несколько шагов, что позволяет более эффективно находить оптимальные искажения

Файл [I_FGSM.py](I_FGSM.py) содержит функцию i_fgsm(), являющейся реализацией нецеловой атаки на изображение с помощью метода FGSM.

Метод был протестирован с различными значениями epsilon: 0.001, 0.005, 0.01, 0.05, 0.1, 0.15 и 10 итерациями, так как при значениях epsilon больше, чем 0.15, наличие шума становится отчетливо визуально отличимо.

Пример изображения при значении epsilon = 0.2
![image](images/i_fgsm/mobilenet/7.png)
