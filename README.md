## Постановка задачи:
Разработка методов генерации adversarial примеров и оценка их воздействия на предобученные нейросетевые модели для классификации и обнаружения людей, с акцентом на анализ изменений в точности(accuracy) моделей при использовании как стандартного тестового набора данных, так и набора данных, состоящего из adversarial примеров, сгененрированных с помощью различных методов атак при различных гиперпараметрах.


### Иследуемые предобученные модели.
Для анализа использовались модели, предобученные на наборе данных ImageNet, из модуля [torchvision](https://pytorch.org/vision/stable/) библиотеки [PyTorch](https://pytorch.org/).

Для задачи обнаружения объектов в модуле доступны следующие модели:
- [**Faster R-CNN**](https://pytorch.org/vision/stable/models/faster_rcnn.html);
- [**RetinaNet**](https://pytorch.org/vision/stable/models/retinanet.html);
- [**SSD**](https://pytorch.org/vision/stable/models/ssd.html);
- [**SSDlite**](https://pytorch.org/vision/stable/models/ssdlite.html).

В backbone моделей, приведенных выше, используютя следующие модели классификации:
- [**ResNet50**](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.resnet50) с механизмом Feature Pyramid Network(FPN) для более эффективного извлечения признаков;
- [**MobileNet V3**](https://pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.mobilenet_v3_large);
- [**VGG-16**](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html#torchvision.models.vgg16).

Было принято решение использовать предобученные на наборе данных [ImageNet](https://image-net.org/index.php) модели классификации, представляющие backbone моделей обнаружения объектов, для проведения исследоваения.


### Тестовый набор данных.
Для оценки точности предобученнах моделей, был собран и аннотирован тестовый набор данных, состоящий из 200 изображений различных классов набора данных ImageNet.

Набор данных представлен в репозитории -- [тестовый набор данных](dataset/)

Папка [images](dataset/images/) содержит изображения, входящие в тестовый набор данных.
Файл [labels.csv](dataset/labels.csv) содержит в себе аннотации к изображениям.  


### Оценка точности предобученных моделей на тестовом наборе данных.
- **ResNet50**<br>
  Точность модели на тестовом наборе: 96.00 %
- **MobileNet V3**<br>
  Точность модели на тестовом наборе: 89.50 %
- **VGG-16**<br>
  Точность модели на тестовом наборе: 83.00 %

#### Пример первых пяти изображений и их классов из тестового набора данных

![Orig images and their classes](images/orig_imgs.png)

### Классификация состязательных атак (Adversarial Attacks)

Состязательная атака (adversarial attack) – это воздействие на нейронную сеть при помощи изменения исходных данных с целью получения неверного результата классификации.

Можно выделить 5 основных критериев классификации состязательных атак:

1. **Результат воздействия на систему**:
   - Изменение поведения;
   - Нарушение конфиденциальности.

2. **Вид атаки**:
   - Атака отравления;
   - Атака уклонения;
   - Атака BadNets.

3. **Наличие информации о модели**:
   - Белый ящик;
   - Черный ящик;
   - Серый ящик.

4. **Тип атаки**:
   - Целевая атака;
   - Нецелевая атака.

5. **Тип применимости**:
   - Виртуальная/цифровая;
   - Физическая.

#### Атаки изменения поведения

Атаки изменения поведения ставят своей целью изменить результат работы модели, то есть нарушить целостность системы. 

#### Атаки нарушения конфиденциальности

Атаки нарушения конфиденциальности ставят своей целью получение доступа к различным артефактам сети, таким как обучающие данные, архитектура, веса обученной модели и т.д. Также сюда относятся атаки, позволяющие узнать что-то об обучающей выборке или получить модель, которая копирует функционал исходной модели.

#### Атака отравления данных

Атака отравления данных – это задача двухуровневой оптимизации по измененным обучающим данным и по ответу обученной модели. Целью данной атаки является изменение ответа обученной модели.

При обучении модели на открытых наборах данных злоумышленники могут легко их «отравить». Злоумышленники заведомо дают ложный ответ в качестве верного, тем самым отравляя обучающую выборку. На открытых наборах данных экспериментально показано, что при полной доступности параметров модели можно внести в нее критические ошибки.

#### Атаки уклонения

Атаки уклонения (evasion attacks) представляют собой методики, с помощью которых злоумышленники могут вводить небольшие возмущения в входные данные, чтобы обмануть модель машинного обучения и заставить её выдать неправильные результаты. Эти атаки являются серьезной угрозой для безопасности и надежности систем, основанных на глубоких нейронных сетях.

#### Типы атак по доступности информации:

- Атаки с белым ящиком (white-box attacks)<br>
Атака, при которой злоумышленник имеет полный доступ к информации о модели, включая архитектуру, веса и данные обучения. В этом случае он может получить и использовать внутреннюю информацию модели, такую как веса модели и данные обучения. 
*Пример*: злоумышленник знает архитектуру нейронной сети, все параметры и имеет доступ к обучающему набору данных.
- Атаки с черным ящиком (black-box attacks)<br>
Атака, при которой злоумышленник не может получить никакую информацию о модели во время процесса генерации враждебных примеров и может использовать только выходные данные модели. В реальных сценариях получения внутренней информации о целевых системах невозможно, поэтому атаки с черным ящиком более применимы.
*Пример*: злоумышленник имеет доступ только к выходным данным модели (например, к предсказанным меткам или вероятностям), но не знает внутренней структуры или параметров модели.
- Атаки "серого ящика" (gray box)<br>
Атаки, сочетающие характеристики атак "белого" и "черного" ящиков. В этом случае атакующий располагает частичной информацией о модели, что делает такие атаки более реалистичными, но все же накладывает некоторые ограничения. Например, может быть известна архитектура модели или информация о данных, на которых модель была обучена, однако отсутствовать доступ к самим данным или обученным весам.

#### Типы атак по целенаправленности:

- Нецелевая атака<br>
Атаки, целью которых является изменение предсказания модели на любой другой класс, отличный от правильного. Эти атаки проще в реализации.
- Целевая атака<br>
Атаки, целью которых является вынудить модель выдать предсказание конкретного класса. Эти атаки сложнее в реализации, так как требуют более точных возмущений.
