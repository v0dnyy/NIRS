## Постановка задачи:

Разработка методов генерации физичеки реализуемых adversarial примеров и аналих эффективности их воздействия на предобученные нейросетевые модели для классификации и обнаружения людей, с акцентом на анализ изменений в точности(accuracy), достоверности классификации моделей при использовании их для задач классификации и обнаружения людей в режиме реального времени.

### Возможные варианты реализации:

- **Реализация атаки уклонения от системы распознования лиц(face recognition system) с помощью очков или масок**;
- **Реализация атаки уклонения от системы распознования людей с помощью верхней одежды со специальным рисунком или наклеенным враждебным патчем**;
- **Реализация атаки уклонения от системы распознования объектов с помощью рамки**.

### Научные статьи/публикации по теме работы:

- **Реализация атаки уклонения от системы распознования лиц**<br>

  [Adversarial Mask: Real-World Universal Adversarial Attack on Face Recognition Models](https://arxiv.org/pdf/2111.10759)
  

- **Реализация атаки уклонения от системы распознования людей**<br>

  [Fooling automated surveillance cameras: adversarial patches to attack person detection](https://arxiv.org/pdf/1904.08653)

  [Adversarial T-shirt! Evading Person Detectors in A Physical World](https://arxiv.org/pdf/1910.11099)
  
- **Реализация универсальных атак уклонения**<br>

  [Adversarial Patch](https://arxiv.org/pdf/1712.09665)

  [Robust Physical-World Attacks on Deep Learning Visual Classification](https://arxiv.org/pdf/1707.08945)

  [Physical Adversarial Examples for Object Detectors](https://arxiv.org/pdf/1807.07769)
  
